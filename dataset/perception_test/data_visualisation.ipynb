{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install opencv-python\n",
        "!pip install imageio\n",
        "!pip install moviepy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Up7CEnJriAVu"
      },
      "outputs": [],
      "source": [
        "# @title Prerequisites\n",
        "import colorsys\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "from typing import Tuple, List, Dict\n",
        "import zipfile\n",
        "\n",
        "import cv2\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import moviepy.editor as mvp\n",
        "import numpy as np\n",
        "import requests\n",
        "from scipy.io import wavfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1q_JZ0ueikxp"
      },
      "outputs": [],
      "source": [
        "# @title Utility Functions\n",
        "def download_and_unzip(url: str, destination: str):\n",
        "  \"\"\"Downloads and unzips a .zip file to a destination.\n",
        "\n",
        "  Downloads a file from the specified URL, saves it to the destination\n",
        "  directory, and then extracts its contents.\n",
        "\n",
        "  If the file is larger than 1GB, it will be downloaded in chunks,\n",
        "  and the download progress will be displayed.\n",
        "\n",
        "  Args:\n",
        "    url (str): The URL of the file to download.\n",
        "    destination (str): The destination directory to save the file and\n",
        "      extract its contents.\n",
        "  \"\"\"\n",
        "  if not os.path.exists(destination):\n",
        "    os.makedirs(destination)\n",
        "\n",
        "  filename = url.split('/')[-1]\n",
        "  file_path = os.path.join(destination, filename)\n",
        "\n",
        "  if os.path.exists(file_path):\n",
        "    print(f'{filename} already exists. Skipping download.')\n",
        "    return\n",
        "\n",
        "  response = requests.get(url, stream=True)\n",
        "  total_size = int(response.headers.get('content-length', 0))\n",
        "  gb = 1024*1024*1024\n",
        "\n",
        "  if total_size / gb > 1:\n",
        "    print(f'{filename} is larger than 1GB, downloading in chunks')\n",
        "    chunk_flag = True\n",
        "    chunk_size = int(total_size/100)\n",
        "  else:\n",
        "    chunk_flag = False\n",
        "    chunk_size = total_size\n",
        "\n",
        "  with open(file_path, 'wb') as file:\n",
        "    for chunk_idx, chunk in enumerate(\n",
        "        response.iter_content(chunk_size=chunk_size)):\n",
        "      if chunk:\n",
        "        if chunk_flag:\n",
        "          print(f\"\"\"{chunk_idx}% downloading\n",
        "          {round((chunk_idx*chunk_size)/gb, 1)}GB\n",
        "          / {round(total_size/gb, 1)}GB\"\"\")\n",
        "        file.write(chunk)\n",
        "  print(f\"'{filename}' downloaded successfully.\")\n",
        "\n",
        "  with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(destination)\n",
        "  print(f\"'{filename}' extracted successfully.\")\n",
        "\n",
        "  os.remove(file_path)\n",
        "\n",
        "\n",
        "def load_db_json(db_file: str) -> Dict:\n",
        "  \"\"\"Loads a JSON file as a dictionary.\n",
        "\n",
        "  Args:\n",
        "    db_file (str): Path to the JSON file.\n",
        "\n",
        "  Returns:\n",
        "    Dict: Loaded JSON data as a dictionary.\n",
        "\n",
        "  Raises:\n",
        "    FileNotFoundError: If the specified file doesn't exist.\n",
        "    TypeError: If the JSON file is not formatted as a dictionary.\n",
        "  \"\"\"\n",
        "  if not os.path.isfile(db_file):\n",
        "    raise FileNotFoundError(f'No such file: {db_file}')\n",
        "\n",
        "  with open(db_file, 'r') as f:\n",
        "    db_file_dict = json.load(f)\n",
        "    if not isinstance(db_file_dict, dict):\n",
        "      raise TypeError('JSON file is not formatted as a dictionary.')\n",
        "    return db_file_dict\n",
        "\n",
        "\n",
        "def load_mp4_to_frames(filename: str) -> np.array:\n",
        "  \"\"\"Loads an MP4 video file and returns its frames as a NumPy array.\n",
        "\n",
        "  Args:\n",
        "    filename (str): Path to the MP4 video file.\n",
        "\n",
        "  Returns:\n",
        "    np.array: Frames of the video as a NumPy array.\n",
        "  \"\"\"\n",
        "  assert os.path.exists(filename), f'File {filename} does not exist.'\n",
        "  cap = cv2.VideoCapture(filename)\n",
        "\n",
        "  vid_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "  width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "\n",
        "  vid_frames = np.empty((vid_frames, height, width, 3), dtype=np.uint8)\n",
        "\n",
        "  idx = 0\n",
        "  while True:\n",
        "    ret, vid_frame = cap.read()\n",
        "    if not ret:\n",
        "      break\n",
        "\n",
        "    vid_frames[idx] = vid_frame\n",
        "    idx += 1\n",
        "\n",
        "  cap.release()\n",
        "  return vid_frames\n",
        "\n",
        "\n",
        "def get_video_frames(data_item: Dict, vid_path: str) -> np.array:\n",
        "  \"\"\"Loads frames of a video specified by an item dictionary.\n",
        "\n",
        "  Assumes format of annotations used in the Perception Test Dataset.\n",
        "\n",
        "  Args:\n",
        "  \tdata_item (Dict): Item from dataset containing metadata.\n",
        "    vid_path (str): Path to the directory containing videos.\n",
        "\n",
        "  Returns:\n",
        "    np.array: Frames of the video as a NumPy array.\n",
        "  \"\"\"\n",
        "  video_file_path = os.path.join(vid_path,\n",
        "                                 data_item['metadata']['video_id']) + '.mp4'\n",
        "  vid_frames = load_mp4_to_frames(video_file_path)\n",
        "  assert data_item['metadata']['num_frames'] == vid_frames.shape[0]\n",
        "  return vid_frames\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fymsuk6HbPsQ"
      },
      "outputs": [],
      "source": [
        "# @title Download Dataset Sample\n",
        "data_path = '/teamspace/s3_connections/perception_test/'\n",
        "video_path = '/teamspace/s3_connections/perception_test/videos/'\n",
        "\n",
        "# sample annotations and videos the visualise the annotations later\n",
        "sample_annot_url = 'https://storage.googleapis.com/dm-perception-test/zip_data/sample_annotations.zip'\n",
        "download_and_unzip(sample_annot_url, data_path)\n",
        "\n",
        "sample_videos_url = 'https://storage.googleapis.com/dm-perception-test/zip_data/sample_videos.zip'\n",
        "download_and_unzip(sample_videos_url, data_path)\n",
        "\n",
        "db_json_path = os.path.join(data_path, 'sample.json')\n",
        "db_dict = load_db_json(db_json_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhLSUAVdUGx2"
      },
      "outputs": [],
      "source": [
        "# @title Visualisation functions\n",
        "def get_colors(num_colors: int) -> Tuple[int, int, int]:\n",
        "  \"\"\"Generate random colormaps for visualizing different objects and points.\n",
        "\n",
        "  Args:\n",
        "    num_colors (int): The number of colors to generate.\n",
        "\n",
        "  Returns:\n",
        "    Tuple[int, int, int]: A tuple of RGB values representing the\n",
        "      generated colors.\n",
        "  \"\"\"\n",
        "  colors = []\n",
        "  for j in np.arange(0., 360., 360. / num_colors):\n",
        "    hue = j / 360.\n",
        "    lightness = (50 + np.random.rand() * 10) / 100.\n",
        "    saturation = (90 + np.random.rand() * 10) / 100.\n",
        "    color = colorsys.hls_to_rgb(hue, lightness, saturation)\n",
        "    color = (int(color[0] * 255), int(color[1] * 255), int(color[2] * 255))\n",
        "    colors.append(color)\n",
        "  random.seed(0)\n",
        "  random.shuffle(colors)\n",
        "  return colors\n",
        "\n",
        "\n",
        "def display_video(vid_frames: np.array, fps: int = 30):\n",
        "  \"\"\"Create and display temporary video from numpy array frames.\n",
        "\n",
        "  Args:\n",
        "    vid_frames: (np.array): The frames of the video as a\n",
        "    \tnumpy array. Format of frames should be:\n",
        "    \t(num_frames, height, width, channels)\n",
        "    fps (int): Frames per second for the video playback. Default is 30.\n",
        "  \"\"\"\n",
        "  kwargs = {'macro_block_size': None}\n",
        "  imageio.mimwrite('tmp_video_display.mp4',\n",
        "                   vid_frames[:, :, :, ::-1], fps=fps, **kwargs)\n",
        "  display(mvp.ipython_display('tmp_video_display.mp4'))\n",
        "\n",
        "\n",
        "def display_frame(tmp_frame: np.array):\n",
        "  \"\"\"Display a frame, converting from RGB to BGR for cv2.\n",
        "\n",
        "  Args:\n",
        "    tmp_frame (np.array): The frame to be displayed.\n",
        "  \"\"\"\n",
        "  cv2_imshow(tmp_frame)\n",
        "\n",
        "\n",
        "def paint_box(video: np.array, track: Dict,\n",
        "\t\tcolor: Tuple[int, int, int] = (255, 0, 0),\n",
        "  \taddn_label: str = '') -> np.array:\n",
        "  \"\"\"Paint bounding box and label on video for a given track.\n",
        "\n",
        "  Args:\n",
        "    video (np.array): The video frames as a numpy array.\n",
        "    track (Dict): The track information containing bounding box\n",
        "    and frame information, assumes Perception Test Dataset format.\n",
        "    color (Tuple[int, int, int]): The RGB color values for the bounding box.\n",
        "      Default is red (255, 0, 0).\n",
        "    addn_label (str): Additional label to be added to the track label.\n",
        "      Default is an empty string.\n",
        "\n",
        "  Returns:\n",
        "    np.array: The modified video frames with painted bounding box and\n",
        "      label.\n",
        "  \"\"\"\n",
        "  _, height, width, _ = video.shape\n",
        "  name = str(track['id']) + ' : ' + track['label'] + addn_label\n",
        "  bounding_boxes = np.array(track['bounding_boxes'])\n",
        "\n",
        "  for box, frame_id in zip(bounding_boxes, track['frame_ids']):\n",
        "    frame = np.array(video[frame_id])\n",
        "    x1 = int(round(box[0] * width))\n",
        "    y1 = int(round(box[1] * height))\n",
        "    x2 = int(round(box[2] * width))\n",
        "    y2 = int(round(box[3] * height))\n",
        "    frame = cv2.rectangle(frame, (x1, y1), (x2, y2),\n",
        "                          color=color, thickness=2)\n",
        "    frame = cv2.putText(frame, name, (x1, y1 + 20),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.75, color, 2)\n",
        "    video[frame_id] = frame\n",
        "\n",
        "  return video\n",
        "\n",
        "\n",
        "def paint_boxes(video: np.array, tracks: List[Dict]) -> np.array:\n",
        "  \"\"\"Paint bounding boxes and labels on a video for multiple tracks.\n",
        "\n",
        "  Args:\n",
        "    video (np.array): The video frames as a numpy array.\n",
        "    tracks (List): A list of track information,\n",
        "      where each track contains bounding box and frame information.\n",
        "\n",
        "  Returns:\n",
        "    np.array: The modified video frames with painted bounding boxes\n",
        "      and labels.\n",
        "  \"\"\"\n",
        "  for track_idx, track in enumerate(tracks):\n",
        "    video = paint_box(video, track, COLORS[track_idx])\n",
        "  return video\n",
        "\n",
        "\n",
        "def paint_point(video: np.array,\n",
        "  \ttrack: dict, color: tuple[int, int, int] = (255, 0, 0)) -> np.array:\n",
        "  \"\"\"Paints a single tracked point on each frame of a video.\n",
        "\n",
        "  Args:\n",
        "    video (np.array): The video frames as a numpy array.\n",
        "    track (dict): The track containing frame IDs and corresponding points.\n",
        "    color (tuple, optional): The color of the painted point.\n",
        "      Defaults to (255, 0, 0).\n",
        "\n",
        "  Returns:\n",
        "    np.array: The video frames with painted points.\n",
        "  \"\"\"\n",
        "  _, height, width, _ = video.shape\n",
        "  for idx, frame_id in enumerate(track['frame_ids']):\n",
        "    vid_frame = video[frame_id]\n",
        "    y = int(round(track['points'][0][idx] * height))\n",
        "    x = int(round(track['points'][1][idx] * width))\n",
        "    vid_frame = cv2.circle(vid_frame, (x, y),\n",
        "    \t\t\t\t\t\tradius=10, color=color, thickness=-1)\n",
        "    video[frame_id] = vid_frame\n",
        "  return video\n",
        "\n",
        "\n",
        "def paint_points(video: np.array, tracks: List[dict]) -> np.array:\n",
        "  \"\"\"Paints multiple tracked points on each frame of a video.\n",
        "\n",
        "  Args:\n",
        "    video (np.array): The video frames as a numpy array.\n",
        "    tracks (List[dict]): The list of tracks containing\n",
        "      frame IDs and corresponding points.\n",
        "\n",
        "  Returns:\n",
        "    np.array: The video frames with painted points.\n",
        "  \"\"\"\n",
        "  for idx, track in enumerate(tracks):\n",
        "    video = paint_point(video, track, COLORS[idx])\n",
        "  return video\n",
        "\n",
        "\n",
        "def paint_sound(video: np.array,\n",
        "    vid_sound: dict, vid_frames: np.array,\n",
        "    color: tuple[int, int, int] = (0, 0, 255)) -> np.array:\n",
        "  \"\"\"Paints a sound label on each frame of a video.\n",
        "\n",
        "  Args:\n",
        "    video (np.array): The video frames as a numpy array.\n",
        "    vid_sound (dict): The sound containing the label,\n",
        "      frame IDs, and visibility.\n",
        "    vid_frames (np.array): The array to keep track of\n",
        "      the number of labels on each frame.\n",
        "    color (tuple, optional): The color of the painted label.\n",
        "      Defaults to (0, 0, 255).\n",
        "\n",
        "  Returns:\n",
        "    np.array: The video frames with painted labels.\n",
        "  \"\"\"\n",
        "  _, height, width, _ = video.shape\n",
        "  name = f\"\"\"Sound: {vid_sound[\"label\"]}\n",
        "  is_visible: {str(bool(vid_sound[\"is_visible\"]))}\"\"\"\n",
        "  [start_frame, end_frame] = vid_sound['frame_ids']\n",
        "  for frame_id in range(start_frame, end_frame):\n",
        "    vid_frame = np.array(video[frame_id])\n",
        "    y1 = int(round(0.9 * height) - (40 * vid_frames[frame_id]))\n",
        "    x1 = int(round(0.05 * width))\n",
        "\n",
        "    vid_frame = cv2.putText(vid_frame, name, (x1, y1),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1.5, color, 2)\n",
        "    video[frame_id] = vid_frame\n",
        "    vid_frames[frame_id] += 1\n",
        "\n",
        "\n",
        "  return video\n",
        "\n",
        "\n",
        "def paint_action(video: np.array, vid_action: dict,\n",
        "\t\tvid_frames: np.array, color: tuple[int, int, int] = (0, 255, 0),\n",
        "    ) -> np.array:\n",
        "  \"\"\"Paints an action label on each frame of a video.\n",
        "\n",
        "  Args:\n",
        "    video (np.array): The video frames as a numpy array.\n",
        "    vid_action (dict): The action containing the label and frame IDs.\n",
        "    vid_frames (np.array): The array to keep track\n",
        "      of the number of labels on each frame.\n",
        "    color (tuple, optional): The color of the painted label.\n",
        "      Defaults to (0, 255, 0).\n",
        "\n",
        "  Returns:\n",
        "    np.array: The video frames with painted labels.\n",
        "  \"\"\"\n",
        "  _, height, width, _ = video.shape\n",
        "  name = f\"\"\"Action: {vid_action[\"label\"]}\"\"\"\n",
        "  [start_frame, end_frame] = vid_action['frame_ids']\n",
        "  for frame_id in range(start_frame, end_frame):\n",
        "    vid_frame = np.array(video[frame_id])\n",
        "    y1 = int(round(0.9 * height) - (40 * vid_frames[frame_id]))\n",
        "    x1 = int(round(0.05 * width))\n",
        "\n",
        "    vid_frame = cv2.putText(vid_frame, name, (x1, y1),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1.5, color, 2)\n",
        "    video[frame_id] = vid_frame\n",
        "    vid_frames[frame_id] += 1\n",
        "\n",
        "  return video\n",
        "\n",
        "\n",
        "def paint_actions(video: np.array,\n",
        "    vid_actions: List[dict], vid_frames: np.array) -> np.array:\n",
        "  \"\"\"Paints multiple action labels on each frame of a video.\n",
        "\n",
        "  Args:\n",
        "    video (np.array): The video frames as a numpy array.\n",
        "    vid_actions (List[dict]): The list of actions containing\n",
        "      the labels and frame IDs.\n",
        "    vid_frames (np.array): The array to keep track\n",
        "      of the number of labels on each frame.\n",
        "\n",
        "  Returns:\n",
        "    np.array: The video frames with painted labels.\n",
        "  \"\"\"\n",
        "  for vid_action in vid_actions:\n",
        "    video = paint_action(video, vid_action, vid_frames)\n",
        "  return video\n",
        "\n",
        "\n",
        "def paint_sounds(video: np.array,\n",
        "  \tvid_sounds: List[dict], vid_frames: np.array) -> np.array:\n",
        "  \"\"\"Paints multiple sound labels on each frame of a video.\n",
        "\n",
        "  Args:\n",
        "    video (np.array): The video frames as a numpy array.\n",
        "    vid_sounds (List[dict]): The list of sounds containing the labels,\n",
        "      frame IDs, and visibility.\n",
        "    vid_frames (np.array): The array to keep track of the\n",
        "      number of labels on each frame.\n",
        "\n",
        "  Returns:\n",
        "    np.array: The video frames with painted labels.\n",
        "  \"\"\"\n",
        "  for sound in vid_sounds:\n",
        "    video = paint_sound(video, sound, vid_frames)\n",
        "  return video\n",
        "\n",
        "\n",
        "def get_answer_tracks(ex_data: dict, goq_ids: List) -> List[dict]:\n",
        "  \"\"\"Filters and retrieves object tracks based on the given object ids.\n",
        "\n",
        "  Args:\n",
        "    ex_data (dict): The data containing object tracking information.\n",
        "    goq_ids (List): The list of IDs to filter tracks.\n",
        "\n",
        "  Returns:\n",
        "    List[dict]: The filtered tracks matching the goq_ids.\n",
        "  \"\"\"\n",
        "  goq_tracks = []\n",
        "  for track in ex_data['object_tracking']:\n",
        "    if track['id'] in goq_ids:\n",
        "      goq_tracks.append(track)\n",
        "  return goq_tracks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSbbsIDQI6Uf"
      },
      "outputs": [],
      "source": [
        "# @title Show Example Annotations\n",
        "video_id = list(db_dict.keys())[6]\n",
        "example_data = db_dict[video_id]\n",
        "\n",
        "print('---------------------------------------------------------------------')\n",
        "print('Tasks annotated for this video: ')\n",
        "for k, v in example_data.items():\n",
        "  if v:\n",
        "    print(f'{k} - available: yes - annotations: {len(v)}')\n",
        "  else:\n",
        "    print(f'{k} - available: no')\n",
        "print('---------------------------------------------------------------------')\n",
        "print('Video Metadata')\n",
        "print('---------------------------------------------------------------------')\n",
        "for k, v in example_data['metadata'].items():\n",
        "  print(f'{k} : {v}')\n",
        "print('---------------------------------------------------------------------')\n",
        "print('Object Tracking data')\n",
        "print('---------------------------------------------------------------------')\n",
        "for k, v in example_data['object_tracking'][0].items():\n",
        "  print(f'{k} : {v}')\n",
        "print('---------------------------------------------------------------------')\n",
        "print('Multiple-Choice VQA')\n",
        "print('---------------------------------------------------------------------')\n",
        "for k, v in example_data['mc_question'][0].items():\n",
        "  print(f'{k} : {v}')\n",
        "print('---------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPJkHKWPEvff"
      },
      "outputs": [],
      "source": [
        "# @title Visualising Object Tracks\n",
        "if example_data['object_tracking']:\n",
        "  frames = get_video_frames(example_data, video_path)\n",
        "\n",
        "  COLORS = get_colors(num_colors=100)\n",
        "  show_all_tracks = True  # @param {type: \"boolean\"}\n",
        "  show_track = 2  # @param {type: \"integer\"}\n",
        "\n",
        "  if show_all_tracks:\n",
        "    frames = paint_boxes(frames, example_data['object_tracking'])\n",
        "  else:\n",
        "    frames = paint_box(frames, example_data['object_tracking'][show_track])\n",
        "\n",
        "  annotated_frames = []\n",
        "  for frame_idx in example_data['object_tracking'][0]['frame_ids']:\n",
        "    annotated_frames.append(frames[frame_idx])\n",
        "\n",
        "  annotated_frames = np.array(annotated_frames)\n",
        "  display_video(annotated_frames, 1)\n",
        "  del frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJESCRiMr9i7"
      },
      "outputs": [],
      "source": [
        "# @title Visualising Point Tracks\n",
        "if example_data['point_tracking']:\n",
        "  frames = get_video_frames(example_data, video_path)\n",
        "  COLORS = get_colors(num_colors=100)\n",
        "  frames = paint_points(frames, example_data['point_tracking'])\n",
        "  display_video(frames, example_data['metadata']['frame_rate'])\n",
        "  del frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lB8XcG86YHQ"
      },
      "outputs": [],
      "source": [
        "# @title Visualising Action Segments\n",
        "if example_data['action_localisation']:\n",
        "  frames = get_video_frames(example_data, video_path)\n",
        "  labelled_frames = np.zeros(frames.shape[0])\n",
        "  frames = paint_actions(frames, example_data['action_localisation'],\n",
        "  \t\t\t\t labelled_frames)\n",
        "  display_video(frames, example_data['metadata']['frame_rate'])\n",
        "  del frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPEft_TFwcnK"
      },
      "outputs": [],
      "source": [
        "# @title Plotting Action Segments\n",
        "if example_data['action_localisation']:\n",
        "  frames = get_video_frames(example_data, video_path)[:,:,:,::-1]\n",
        "\n",
        "  action_labels = []\n",
        "  action_start_times = []\n",
        "  action_end_times = []\n",
        "\n",
        "  for action in example_data['action_localisation']:\n",
        "    action_labels.append(action['label'])\n",
        "    action_start_times.append(action['timestamps'][0]/1e6)\n",
        "    action_end_times.append(action['timestamps'][1]/1e6)\n",
        "\n",
        "  action_start_times = np.array(action_start_times)\n",
        "  action_end_times = np.array(action_end_times)\n",
        "\n",
        "  plt.figure(figsize=(20, 15))\n",
        "  # Strip of frames\n",
        "  plt.subplot(4, 1, 2)\n",
        "  plt.title('Video Frames')\n",
        "  f_size = frames[0].shape\n",
        "  small = tuple(reversed((np.array(f_size[:2]) / 4).astype(int)))\n",
        "  strip = None\n",
        "  num_frames = example_data['metadata']['num_frames']\n",
        "  for i in range(0, num_frames, int(num_frames/4)):\n",
        "    frame = cv2.resize(frames[i], small)\n",
        "    if strip is None:\n",
        "      strip = np.array(frame)\n",
        "    else:\n",
        "      strip = np.concatenate([strip, frame], axis=1)\n",
        "    plt.imshow(strip)\n",
        "\n",
        "  del frames\n",
        "\n",
        "  plt.subplot(4, 1, 3)\n",
        "  plt.title('Action Events')\n",
        "  plt.barh(range(len(action_start_times)),\n",
        "           action_end_times-action_start_times,\n",
        "           left=action_start_times)\n",
        "  plt.yticks(range(len(action_start_times)), action_labels)\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RR9Mm_0T6iBn"
      },
      "outputs": [],
      "source": [
        "# @title Visualising Multiple-Choice Video Question-Answering Annotations\n",
        "if example_data['mc_question']:\n",
        "  for question in example_data['mc_question']:\n",
        "    print('---------------------------------')\n",
        "    print('Question: ', question['question'])\n",
        "    print('Options: ', question['options'])\n",
        "    print('Answer ID: ', question['answer_id'])\n",
        "    print('Answer: ', question['options'][question['answer_id']])\n",
        "    print('Question info: ')\n",
        "    print('Reasoning: ', question['reasoning'])\n",
        "    print('Tag: ', question['tag'])\n",
        "    print('area: ', question['area'])\n",
        "    print('---------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iFMOZgShAqn"
      },
      "outputs": [],
      "source": [
        "# @title Visualising Grounded Video Question-Answering\n",
        "\n",
        "# loading an example that has grounded question annotations\n",
        "video_id = list(db_dict.keys())[7]\n",
        "example_data = db_dict[video_id]\n",
        "\n",
        "if example_data['grounded_question']:\n",
        "  question = example_data['grounded_question'][0]\n",
        "  print('---------------------------------')\n",
        "  print('Question: ', question['question'])\n",
        "  print('Answer IDs: ', question['answers'])\n",
        "  print('Question info: ')\n",
        "  print('Reasoning: ', question['reasoning'])\n",
        "  print('area: ', question['area'])\n",
        "  print('---------------------------------')\n",
        "\n",
        "  frames = get_video_frames(example_data, video_path)\n",
        "  answer_tracks = get_answer_tracks(example_data, question['answers'])\n",
        "  frames = paint_boxes(frames, answer_tracks)\n",
        "\n",
        "  annotated_frames = []\n",
        "  for frame_idx in answer_tracks[0]['frame_ids']:\n",
        "    annotated_frames.append(frames[frame_idx])\n",
        "\n",
        "  annotated_frames = np.array(annotated_frames)\n",
        "  display_video(annotated_frames, 1)\n",
        "  del frames"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
